# -*- coding: utf-8 -*-
'''
Utils script for the structural RNN implementation
Handles processing the input and target data in batches and sequences

Author :Yuexin Ma
Date : May 2018
'''

import os
import pickle
import numpy as np
import random
import pdb
import sys
# reload(sys)
# sys.setdefaultencoding("utf-8")

class DataLoader():

    def __init__(self, batch_size=50, seq_length=12, datasets=[0, 1, 2],choose_test=False, forcePreProcess=False, infer=False):
        '''
        Initialiser function for the DataLoader class
        params:
        batch_size : Size of the mini-batch
        seq_length : Sequence length to be considered
        datasets : The indices of the datasets to use
        forcePreProcess : Flag to forcefully preprocess the data again from csv files
        '''
        # List of data directories where raw data resides
        self.data_dirs = ['./data/pickle0/pickle0/rosbag_merged_BYD004_20171016144948_1508136911_1508136959.pkl',
                           './data/pickle0/pickle0/rosbag_merged_BYD004_20171016144948_1508137023_1508137835.pkl',
                           './data/pickle0/pickle0/rosbag_merged_BYD004_20171016144948_1508138005_1508138177.pkl']
                           #'./data/pickle0/pickle0/rosbag_merged_MKZ044_20171026135134_1508998056_1508999255',
                           #'./data/pickle0/pickle0/rosbag_merged_MKZ044_20171026135134_1508999256_1508999679',
                           #'./data/pickle0/pickle0/rosbag_merged_MKZ044_20171026135134_1509000741_1509001052',
                           #'./data/pickle0/pickle0/rosbag_merged_MKZ044_20171026135134_1509003017_1509004216'
        #                   './data/pickle0/pickle0/rosbag_merged_MKZ044_20171026135134_1509004217_1509004564',
        #                   './data/pickle0/pickle0/rosbag_merged_MKZ046_20171011112354_1507692497_1507692979',
        #                   './data/pickle0/pickle0/rosbag_merged_MKZ046_20171011112354_1507693048_1507693354',
        #                   './data/pickle0/pickle0/rosbag_merged_MKZ046_20171011112354_1507693487_1507693918',
        #                   './data/pickle0/pickle0/rosbag_merged_MKZ046_20171011112354_1507693943_1507694148',
        #                   './data/pickle0/pickle0/rosbag_merged_MKZ046_20171011140740_1507702525_1507702676']


        #self.used_data_dirs = [self.data_dirs[x] for x in datasets]
        self.infer = infer

        # Number of datasets
        self.numDatasets = len(self.data_dirs)

        # Data directory where the pre-processed pickle file resides
        self.data_dir = 'data'

        # Store the arguments
        self.batch_size = batch_size
        self.seq_length = seq_length
        self.choose_test = choose_test

        # Validation arguments
        self.val_fraction = 0.2

        # Define the path in which the process data would be stored
        data_file = os.path.join(self.data_dir, "dense_type2_gap4_normal_2.cpkl")
        #pdb.set_trace()
        # If the file doesn't exist or forcePreProcess is true
        if not(os.path.exists(data_file)) or forcePreProcess:
            print("Creating pre-processed data from raw data")
            # Preprocess the data from the csv files of the datasets
            # Note that this data is processed in frames
            self.frame_preprocess(self.used_data_dirs, data_file)

        # Load the processed data from the pickle file
        if choose_test:
            self.load_test_preprocessed(data_file)
        else:
            self.load_preprocessed(data_file)
        # Reset all the data pointers of the dataloader object
        self.reset_batch_pointer(valid=False)
        self.reset_batch_pointer(valid=True)

    def frame_preprocess(self, data_dirs, data_file):
        print(data_file)
        '''
        Function that will pre-process the pixel_pos.csv files of each dataset
        into data with occupancy grid that can be used
        params:
        data_dirs : List of directories where raw data resides
        data_file : The file into which all the pre-processed data needs to be stored
        '''
        # all_frame_data would be a list of list of numpy arrays corresponding to each dataset
        # Each numpy array will correspond to a frame and would be of size (numPeds, 3) each row
        # containing pedID, x, y
        all_frame_data = []
        # Validation frame data
        valid_frame_data = []
        # frameList_data would be a list of lists corresponding to each dataset
        # Each list would contain the frameIds of all the frames in the dataset
        frameList_data = []
        # numPeds_data would be a list of lists corresponding to each dataset
        # Ech list would contain the number of pedestrians in each frame in the dataset
        numPeds_data = []
        # Index of the current dataset
        dataset_index = 0

        # For each dataset
        for directory in data_dirs:
            # define path of the csv file of the current dataset
            # file_path = os.path.join(directory, 'pixel_pos.csv')
            file_path = directory

            print('file:   ', file_path)
            # Load the data
            f = open(file_path, 'rb')
            raw_data = pickle.load(f)
            f.close()

            frameList = [raw_data[0]['timestamp']]
            max_x = raw_data[0]['position'][0]
            max_y = raw_data[0]['position'][1]
            min_x = raw_data[0]['position'][0]
            min_y = raw_data[0]['position'][1]
            max_type = raw_data[0]['type']
            min_type = raw_data[0]['type']
            for i in range(len(raw_data)):
                if frameList.count(raw_data[i]['timestamp'])==0:
                    frameList.append(raw_data[i]['timestamp'])

                if raw_data[i]['position'][0] < min_x:
                    min_x = raw_data[i]['position'][0]
                if raw_data[i]['position'][0] > max_x:
                    max_x = raw_data[i]['position'][0]
                if raw_data[i]['position'][1] < min_y:
                    min_y = raw_data[i]['position'][1]
                if raw_data[i]['position'][1] > max_y:
                    max_y = raw_data[i]['position'][1]

            gap_x = max_x - min_x
            gap_y = max_y - min_y

            print('boundary: {}:{}:{}:{}'.format(min_x, max_x, min_y, max_y))

            #print('frameSet: {}'.format(frameList))

            # Frame IDs of the frames in the current dataset
            numFrames = len(frameList)

           # print('All data List size:', len(raw_data))
           # print('frameList size:',numFrames)

            # Add the list of frameIDs to the frameList_data
            frameList_data.append(frameList)
            # Initialize the list of numPeds for the current dataset
            numPeds_data.append([])
            # Initialize the list of numpy arrays for the current dataset
            all_frame_data.append([])
            # Initialize the list of numpy arrays for the current dataset
            valid_frame_data.append([])

            # skip = 10
            counter = 0
            for ind, frame in enumerate(frameList):

                # # NOTE CHANGE
                # if ind % skip != 0:
                #     # SKip every n frames
                #     continue

                #print('ind, frame {}:{}'.format(ind, frame))

                pedsInFrame = []

                # Extract all pedestrians in current frame
                for r in range(counter,len(raw_data)):
                    if raw_data[r]['timestamp'] == frame:
                        pedsInFrame.append(raw_data[r])
                        counter += 1
                    else:
                        break

                #print('pedsInFrame size: ',len(pedsInFrame))

                pedsWithPos = []

                for ped in range(len(pedsInFrame)):
                    current_id = pedsInFrame[ped]['id']
                    if current_id==-1:
                       # print('!!!!!!!!!!:-1')
                        continue
                    current_type = pedsInFrame[ped]['type']
                    current_position = pedsInFrame[ped]['position']
                    current_velocity = pedsInFrame[ped]['velocity']
                    current_width = pedsInFrame[ped]['width']
                    current_length = pedsInFrame[ped]['length']
                    current_heading = pedsInFrame[ped]['heading']
                    #print(current_type)
                    # if pedRecords.count(current_id):
                    #     print("!!!!!!!!!!!!!!!!!!!!!!!!!")

                    # Add their pedID, type, position, velocity, width, length, heading to the row of the numpy array
                    pedsWithPos.append([current_id, (current_type)/5.0, (current_position[0]-min_x)/gap_x, (current_position[1]-min_y)/gap_y, current_position[2],current_velocity[0], current_velocity[1], current_velocity[2], current_width,current_length, current_heading])

                    #print(pedsWithPos[-1][0], pedsWithPos[-1][1], pedsWithPos[-1][2], pedsWithPos[-1][3])

                numPeds_data[dataset_index].append(len(pedsInFrame))

                if (ind > numFrames * self.val_fraction) or (self.infer):
                    # At inference time, no validation data
                    # Add the details of all the peds in the current frame to all_frame_data
                    all_frame_data[dataset_index].append(np.array(pedsWithPos))
                else:
                    valid_frame_data[dataset_index].append(np.array(pedsWithPos))

            dataset_index += 1

        # Save the tuple (all_frame_data, frameList_data, numPeds_data) in the pickle file
        f = open(data_file, "wb")
        pickle.dump((all_frame_data, frameList_data, numPeds_data, valid_frame_data), f, protocol=2)
        f.close()


    def load_test_preprocessed(self, data_file):
        '''
        Function to load the pre-processed data into the DataLoader object
        params:
        data_file : the path to the pickled data file
        '''
        print('load test preprocessed......{}'.format(data_file))
        # Load data from the pickled file
        f = open(data_file, 'rb')
        self.raw_data = pickle.load(f)
        f.close()
        # Get all the data from the pickle file
        self.data = self.raw_data[0]
        self.frameList = self.raw_data[1]
        self.numPedsList = self.raw_data[2]
        self.valid_data = self.data
        self.test_data = len(self.data)*3/4
        self.data = self.data[self.test_data:len(self.data)]
        #self.data = self.data[len(self.data)/4: len(self.data)/2]

        counter = 0
        valid_counter = 0

        # For each dataset
        for dataset in range(len(self.data)):
            # get the frame data for the current dataset
            all_frame_data = self.data[dataset]
            valid_frame_data = self.valid_data[dataset]
            #print('Test data from dataset', dataset, ':', len(all_frame_data))
            #print('Validation data from dataset', dataset, ':', len(valid_frame_data))
            # Increment the counter with the number of sequences in the current dataset
            counter += int(len(all_frame_data) / (self.seq_length))
            #print('{}:{}'.format(dataset,counter))
            valid_counter += int(len(valid_frame_data) / (self.seq_length))

        # Calculate the number of batches
        self.num_batches = int(counter/self.batch_size)
        #self.valid_num_batches = int(valid_counter/self.batch_size)

        print('seq, counter, dataset: {}:{}:{}'.format(self.seq_length,counter,len(self.data)))
        print('Total number of test batches:', self.num_batches )
        #print('Total number of validation batches:', self.valid_num_batches)
        # On an average, we need twice the number of batches to cover the data
        # due to randomization introduced
        #pdb.set_trace()



    def load_preprocessed(self, data_file):
        '''
        Function to load the pre-processed data into the DataLoader object
        params:
        data_file : the path to the pickled data file
        '''
        print('load preprocessed......{}'.format(data_file))
        # Load data from the pickled file
        f = open(data_file, 'rb')
        self.raw_data = pickle.load(f)
        f.close()
        # Get all the data from the pickle file
        self.data = self.raw_data[0]
        self.frameList = self.raw_data[1]
        self.numPedsList = self.raw_data[2]
        self.data = self.data[0:len(self.data)*3/4]

        #pdb.set_trace()
        self.valid_data = self.data

        counter = 0
        valid_counter = 0

        # For each dataset
        for dataset in range(len(self.data)):
            # get the frame data for the current dataset
            all_frame_data = self.data[dataset]
            valid_frame_data = self.valid_data[dataset]
            #print('Training data from dataset', dataset, ':', len(all_frame_data))
            #print('Validation data from dataset', dataset, ':', len(valid_frame_data))
            # Increment the counter with the number of sequences in the current dataset
            counter += int(len(all_frame_data) / (self.seq_length))
            #print('{}:{}'.format(dataset,counter))

            valid_counter += int(len(valid_frame_data) / (self.seq_length))

        # Calculate the number of batches
        self.num_batches = int(counter/self.batch_size)
        self.valid_num_batches = int(counter/3/self.batch_size)
        self.test_data = len(self.data)
        #pdb.set_trace()
        print('seq, counter, dataset: {}:{}:{}'.format(self.seq_length,counter,len(self.data)))
        print('Total number of training batches:', self.num_batches*2 )
        #print('Total number of validation batches:', self.valid_num_batches)
        # On an average, we need twice the number of batches to cover the data
        # due to randomization introduced
        self.num_batches = self.num_batches*2
        self.valid_num_batches = self.valid_num_batches

    def next_batch(self, randomUpdate=True):
        '''
        Function to get the next batch of points
        '''
        # Source data
        x_batch = []
        # Target data
        y_batch = []

        frame_batch = []
        # Dataset data
        d = []
        # Iteration index
        i = 0
        while i < self.batch_size:
            # Extract the frame data of the current dataset
            frame_data = self.data[self.dataset_pointer]
            frame_ids = self.frameList[self.dataset_pointer]
            # Get the frame pointer for the current dataset
            idx = self.frame_pointer
            # While there is still seq_length number of frames left in the current dataset
            if idx + self.seq_length < len(frame_data):
                # All the data in this sequence
                # seq_frame_data = frame_data[idx:idx+self.seq_length+1]
                seq_source_frame_data = frame_data[idx:idx+self.seq_length]
                seq_target_frame_data = frame_data[idx+1:idx+self.seq_length+1]
                #seq_frame_ids = frame_ids[idx:idx+self.seq_length]
                #pdb.set_trace()
                # Number of unique peds in this sequence of frames
                x_batch.append(seq_source_frame_data)
                y_batch.append(seq_target_frame_data)
                #frame_batch.append(seq_frame_ids)

                #randomUpdate = False
                # advance the frame pointer to a random point
                if randomUpdate:
                    self.frame_pointer += random.randint(1, self.seq_length)
                else:
                    self.frame_pointer += self.seq_length

                d.append(self.dataset_pointer)
                i += 1

            else:
                # Not enough frames left
                # Increment the dataset pointer and set the frame_pointer to zero
                self.tick_batch_pointer(valid=False)

        return x_batch, y_batch, d

    def next_valid_batch(self, randomUpdate=True):
        '''
        Function to get the next Validation batch of points
        '''
        # Source data
        x_batch = []
        # Target data
        y_batch = []
        # Dataset data
        d = []
        # Iteration index
        i = 0
        while i < self.batch_size:
            # Extract the frame data of the current dataset
            frame_data = self.data[self.valid_dataset_pointer]
            # Get the frame pointer for the current dataset
            idx = self.valid_frame_pointer
            # While there is still seq_length number of frames left in the current dataset
            if idx + self.seq_length < len(frame_data):
                # All the data in this sequence
                # seq_frame_data = frame_data[idx:idx+self.seq_length+1]
                seq_source_frame_data = frame_data[idx:idx+self.seq_length]
                seq_target_frame_data = frame_data[idx+1:idx+self.seq_length+1]

                # Number of unique peds in this sequence of frames
                x_batch.append(seq_source_frame_data)
                y_batch.append(seq_target_frame_data)

                # advance the frame pointer to a random point
                if randomUpdate:
                    self.valid_frame_pointer += random.randint(1, self.seq_length)
                else:
                    self.valid_frame_pointer += self.seq_length

                d.append(self.valid_dataset_pointer)
                i += 1

            else:
                # Not enough frames left
                # Increment the dataset pointer and set the frame_pointer to zero
                self.tick_batch_pointer(valid=True)

        return x_batch, y_batch, d

    def tick_batch_pointer(self, valid=False):
        '''
        Advance the dataset pointer
        '''

        if not valid:
            # Go to the next dataset
            self.dataset_pointer += 1
            # Set the frame pointer to zero for the current dataset
            self.frame_pointer = 0
            # If all datasets are done, then go to the first one again

            if self.dataset_pointer >= len(self.data):
                self.dataset_pointer = 0


    def reset_batch_pointer(self, valid=False):
        '''
        Reset all pointers
        '''
        if not valid:
            # Go to the first frame of the first dataset
            self.dataset_pointer = 0
            self.frame_pointer = 0


import torch.nn.init as initer
import torch.nn as nn

def init_weights(model, conv='kaiming', batchnorm='normal', linear='kaiming', lstm='kaiming'):
    """
    :param model: Pytorch Model which is nn.Module
    :param conv:  'kaiming' or 'xavier'
    :param batchnorm: 'normal' or 'constant'
    :param linear: 'kaiming' or 'xavier'
    :param lstm: 'kaiming' or 'xavier'
    """
    for m in model.modules():
        if isinstance(m, (nn.Conv1d, nn.Conv2d, nn.Conv3d)):
            if conv == 'kaiming':
                initer.kaiming_normal(m.weight)
            elif conv == 'xavier':
                initer.xavier_normal(m.weight)
            else:
                raise ValueError("init type of conv error.\n")
            if m.bias is not None:
                initer.constant(m.bias, 0)

        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            if batchnorm == 'normal':
                initer.normal(m.weight, 1.0, 0.02)
            elif batchnorm == 'constant':
                initer.constant(m.weight, 1.0)
            else:
                raise ValueError("init type of batchnorm error.\n")
            initer.constant(m.bias, 0.0)

        elif isinstance(m, nn.Linear):
            if linear == 'kaiming':
                initer.kaiming_normal(m.weight)
            elif linear == 'xavier':
                initer.xavier_normal(m.weight)
            else:
                raise ValueError("init type of linear error.\n")
            if m.bias is not None:
                initer.constant(m.bias, 0)

        elif isinstance(m, nn.LSTM):
            for name, param in m.named_parameters():
                if 'weight' in name:
                    if lstm == 'kaiming':
                        initer.kaiming_normal(param)
                    elif lstm == 'xavier':
                        initer.xavier_normal(param)
                    else:
                        raise ValueError("init type of lstm error.\n")
                elif 'bias' in name:
                    initer.constant(param, 1)

#
# if __name__ == '__main__':
#     load = DataLoader()

